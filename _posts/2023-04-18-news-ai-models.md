---
layout: post
title:  "Beyond the Giants: OpenAI's CEO Hints at a New Era in AI Development"
author: sam
categories: [News]
image: assets/images/ai-news-ai-models.svg
description: OpenAI's CEO, Sam Altman, suggests that the future of AI lies in innovative research strategies rather than simply scaling up existing models.
---

So, I was just reading this news piece about OpenAI's CEO, Sam Altman, saying that the age of giant AI models is basically over. You know, the ones like ChatGPT that have been causing quite a stir lately. He mentioned that we'll need to come up with new ideas instead of just making these models bigger and bigger.

OpenAI has been doing some amazing stuff with AI that works with language, like GPT-4, which was probably trained using trillions of words of text and thousands of computer chips. This whole thing cost over $100 million. But Sam Altman believes that making models even larger isn't going to cut it anymore.

It's kinda surprising, especially since Microsoft used ChatGPT's technology to add a chatbot to Bing, and Google launched its own chatbot called Bard. Plus, a bunch of well-funded startups are throwing tons of resources at building massive algorithms to catch up with OpenAI.

So, it seems like GPT-4 might be the last big advance to come from OpenAI's strategy of making models bigger and feeding them more data. Nobody knows yet what kind of research strategies or techniques might replace it. Even Altman says there are physical limits to how many data centers they can build and how fast they can do it.

Nick Frosst, a guy who cofounded Cohere and used to work on AI at Google, agrees with Altman that just going bigger won't work forever. He thinks there are other ways of improving transformers, like new AI model designs or fine-tuning based on human feedback. A lot of researchers are already looking into this.

A quick recap: OpenAI's language algorithms are made up of artificial neural networks, which are trained to predict the words that should follow a given string of text. The first model, GPT-2, had 1.5 billion parameters, and GPT-3 had a mind-blowing 175 billion. This led other companies and research institutions to build their own huge AI models.

People were thinking that GPT-4 would be even more massive, but when it was finally announced, OpenAI didn't even mention how big it is. Maybe because size isn't everything? Sam Altman admitted that training GPT-4 cost more than $100 million, though.

It's possible that GPT-4's intelligence comes from something other than just scale. One idea is reinforcement learning with human feedback, which was used to improve ChatGPT. Humans judge the quality of the model's answers, guiding it towards better responses.

GPT-4's capabilities have amazed experts and sparked debates about AI transforming the economy, spreading disinformation, and taking away jobs. Some AI experts, tech entrepreneurs like Elon Musk, and scientists even wrote an open letter asking for a six-month pause on developing anything more powerful than GPT-4.

Altman confirmed that [OpenAI](https://openai.com/) isn't working on GPT-5 right now and won't be for some time. So, it looks like we're moving into a new era in AI, and it's going to be interesting to see what comes next.
